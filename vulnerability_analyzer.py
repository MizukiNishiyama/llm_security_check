#!/usr/bin/env python3
import json
import os
import argparse
import glob
import random
import time
from datetime import datetime
from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor
from google import genai
from groq import Groq
from openai import OpenAI
from dotenv import load_dotenv


class VulnerabilityAnalyzer:
    def __init__(self, model: str = "gemini", prompt_version: int = 0):
        load_dotenv()
        self.model = model
        self.prompt_version = prompt_version

        # Select prompt based on version
        if prompt_version > 0:
            # Try to get prompt from environment variable PROMPT_N
            env_var = f"PROMPT_{prompt_version}"
            self.prompt = os.environ.get(env_var)
            if self.prompt is None:
                print(f"Warning: Environment variable {env_var} not found, using default prompt")
                self.prompt = "以下のコードを分析し、脆弱性がある場合は1,ない場合は0を出力せよ。必ず数値のみ出力せよ。説明は一切不要である。"
        else:
            # Default prompt (version 0)
            self.prompt = "以下のコードを分析し、脆弱性がある場合は1,ない場合は0を出力せよ。必ず数値のみ出力せよ。説明は一切不要である。"

        if model == "gemini":
            self.client = genai.Client()
        elif model in ["gpt-oss-120b", "gpt-oss-20b"]:
            self.groq_client = Groq(api_key=os.environ.get("GROQ_API_KEY"))
        elif model in ["gpt5", "gpt-5-mini"]:
            self.openai_client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    
    def get_json_files(self, directory: str) -> List[str]:
        pattern = os.path.join(directory, "**", "*.json")
        return glob.glob(pattern, recursive=True)

    def get_c_files(self, directory: str) -> List[str]:
        pattern = os.path.join(directory, "*.c")
        return glob.glob(pattern)
    
    def analyze_code(self, code: str) -> Tuple[int, float]:
        try:
            full_prompt = f"{self.prompt}\n\n{code}"
            start_time = time.time()

            if self.model == "gemini":
                response = self.client.models.generate_content(
                    model="gemini-2.5-flash", contents=full_prompt
                )
                result = response.text.strip()
            elif self.model in ["gpt-oss-120b", "gpt-oss-20b"]:
                chat_completion = self.groq_client.chat.completions.create(
                    messages=[
                        {
                            "role": "user",
                            "content": full_prompt,
                        }
                    ],
                    model="openai/gpt-oss-20b",
                    stream=False,
                )
                result = chat_completion.choices[0].message.content.strip()
            elif self.model == "gpt5":
                response = self.openai_client.responses.create(
                    model="gpt-5",
                    input=full_prompt,
                    reasoning={"effort": "low"},
                    text={"verbosity": "low"},
                )
                result = response.output_text.strip()
            elif self.model == "gpt-5-mini":
                response = self.openai_client.responses.create(
                    model="gpt-5-mini",
                    input=full_prompt,
                    reasoning={"effort": "low"},
                    text={"verbosity": "low"},
                )
                result = response.output_text.strip()
            else:
                print(f"Unsupported model: {self.model}")
                return 0, 0.0

            analysis_time = time.time() - start_time

            if result == "1":
                return 1, analysis_time
            elif result == "0":
                return 0, analysis_time
            else:
                print(f"Unexpected response: {result}")
                return 0, analysis_time
        except Exception as e:
            analysis_time = time.time() - start_time if 'start_time' in locals() else 0.0
            print(f"Error analyzing code: {e}")
            return 0, analysis_time
    
    def process_json_file(self, file_path: str) -> Dict:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            vulnerable_code = data.get("functions", {}).get("vulnerable", {}).get("code", "")

            if not vulnerable_code:
                return {"file": file_path, "result": 0, "error": "No vulnerable code found", "analysis_time": 0.0}

            result, analysis_time = self.analyze_code(vulnerable_code)
            return {"file": file_path, "result": result, "error": None, "analysis_time": analysis_time}

        except Exception as e:
            return {"file": file_path, "result": 0, "error": str(e), "analysis_time": 0.0}

    def process_c_file(self, file_path: str) -> Dict:
        try:
            import re
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Check if this is a normalized file (doesn't have func_0)
            if "normalized" in file_path:
                # For normalized files, analyze the entire file content
                result, analysis_time = self.analyze_code(content)
                return {"file": file_path, "result": result, "error": None, "analysis_time": analysis_time}

            # Extract func_0 function - handle both patterns:
            # 1. void func_0(void)
            # 2. void ClassName::func_0(void)
            pattern = r'void\s+(?:\w+::)?func_0\s*\(\s*void\s*\)'
            match = re.search(pattern, content)

            if not match:
                return {"file": file_path, "result": 0, "error": "func_0 function not found", "analysis_time": 0.0}

            func_0_start = match.start()

            # Find the opening brace after the function declaration
            func_start_brace = content.find("{", match.end())
            if func_start_brace == -1:
                return {"file": file_path, "result": 0, "error": "Opening brace not found for func_0", "analysis_time": 0.0}

            # Find the matching closing brace
            brace_count = 0
            i = func_start_brace
            while i < len(content):
                if content[i] == '{':
                    brace_count += 1
                elif content[i] == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        func_0_code = content[func_0_start:i+1]
                        break
                i += 1
            else:
                return {"file": file_path, "result": 0, "error": "Closing brace not found for func_0", "analysis_time": 0.0}

            result, analysis_time = self.analyze_code(func_0_code)
            return {"file": file_path, "result": result, "error": None, "analysis_time": analysis_time}

        except Exception as e:
            return {"file": file_path, "result": 0, "error": str(e), "analysis_time": 0.0}
    
    def run_analysis(self, directory: str, max_files: int, use_random: bool = False, max_workers: int = 1, cwe78_only: bool = False, analysis_type: str = "original") -> None:
        if analysis_type == "modified_1":
            # Process C files in the specified directory
            target_directory = "data/juliet_modified/1_Ghidra_LLM_CWE78_renamed_v4/bad"
            files_to_process = self.get_c_files(target_directory)

            if not files_to_process:
                print(f"No C files found in {target_directory}")
                return

            print(f"Processing {len(files_to_process)} C files for func_0 functions...")

        elif analysis_type == "v4_extracted":
            # Process JSON files in the v4 extracted directory
            target_directory = "data/juliet_anonymized_extracted/v4"
            json_files = self.get_json_files(target_directory)

            if not json_files:
                print(f"No JSON files found in {target_directory}")
                return

            if use_random and max_files > 0:
                files_to_process = random.sample(json_files, min(max_files, len(json_files)))
            else:
                files_to_process = json_files[:max_files] if max_files > 0 else json_files

            print(f"Processing {len(files_to_process)} v4 extracted JSON files...")

        elif analysis_type == "normalized_gpt5":
            # Process normalized C files in the gpt5 directory
            target_directory = "data/juliet_modified/1_Ghidra_LLM_CWE78_renamed_v4/normalized/gpt5"
            files_to_process = self.get_c_files(target_directory)

            if not files_to_process:
                print(f"No C files found in {target_directory}")
                return

            if use_random and max_files > 0:
                files_to_process = random.sample(files_to_process, min(max_files, len(files_to_process)))
            else:
                files_to_process = files_to_process[:max_files] if max_files > 0 else files_to_process

            print(f"Processing {len(files_to_process)} normalized GPT-5 C files...")

        else:
            # Original processing logic
            json_files = self.get_json_files(directory)

            if cwe78_only:
                json_files = [f for f in json_files if "CWE78_OS_Command_Injection" in f]
                print("Filtering to CWE78_OS_Command_Injection files only")

            if not json_files:
                print(f"No JSON files found in {directory}")
                return

            if use_random and max_files > 0:
                files_to_process = random.sample(json_files, min(max_files, len(json_files)))
            else:
                files_to_process = json_files[:max_files] if max_files > 0 else json_files
        
        print(f"Processing {len(files_to_process)} files...")
        if use_random and analysis_type not in ["modified_1"]:
            print("Files selected randomly")
        if max_workers > 1:
            print(f"Using {max_workers} parallel workers")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Build folder name with options
        folder_parts = [timestamp, analysis_type, f"model-{self.model}"]

        # Add prompt version if not default
        if self.prompt_version > 0:
            folder_parts.append(f"prompt-{self.prompt_version}")

        # Add additional options for original and v4_extracted modes
        if analysis_type in ["original", "v4_extracted"]:
            if cwe78_only:
                folder_parts.append("cwe78only")
            if use_random:
                folder_parts.append("random")
            if max_files > 0:
                folder_parts.append(f"limit-{max_files}")

        if max_workers > 1:
            folder_parts.append(f"parallel-{max_workers}")

        folder_name = "_".join(folder_parts)
        output_dir = f"./output/{folder_name}"
        os.makedirs(output_dir, exist_ok=True)

        results = []
        vulnerable_count = 0
        total_analysis_time = 0.0
        successful_analyses = 0

        # Choose the appropriate processing function
        if analysis_type in ["modified_1", "normalized_gpt5"]:
            process_function = self.process_c_file
        else:
            process_function = self.process_json_file

        if max_workers > 1:
            # 並列処理
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                print("Starting parallel processing...")
                future_to_file = {executor.submit(process_function, file_path): file_path
                                for file_path in files_to_process}

                for i, future in enumerate(future_to_file, 1):
                    file_path = future_to_file[future]
                    print(f"Completed file {i}/{len(files_to_process)}: {os.path.basename(file_path)}")
                    result = future.result()
                    results.append(result)

                    if result["result"] == 1:
                        vulnerable_count += 1

                    # Track analysis time
                    if result.get("analysis_time", 0) > 0:
                        total_analysis_time += result["analysis_time"]
                        successful_analyses += 1
        else:
            # 逐次処理
            for i, file_path in enumerate(files_to_process, 1):
                print(f"Processing file {i}/{len(files_to_process)}: {os.path.basename(file_path)}")
                result = process_function(file_path)
                results.append(result)

                if result["result"] == 1:
                    vulnerable_count += 1

                # Track analysis time
                if result.get("analysis_time", 0) > 0:
                    total_analysis_time += result["analysis_time"]
                    successful_analyses += 1
        
        # Calculate average analysis time
        average_analysis_time = total_analysis_time / successful_analyses if successful_analyses > 0 else 0.0

        report_path = os.path.join(output_dir, "report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"Vulnerability Analysis Report\n")
            f.write(f"Generated at: {datetime.now().isoformat()}\n")
            f.write(f"Model: {self.model}\n")
            f.write(f"Prompt version: {self.prompt_version}\n")
            if self.prompt_version > 0:
                f.write(f"Prompt used: {self.prompt}\n")
            f.write(f"Analysis type: {analysis_type}\n")
            f.write(f"Total files processed: {len(results)}\n")
            f.write(f"Files with vulnerabilities: {vulnerable_count}\n")
            f.write(f"Vulnerability rate: {(vulnerable_count / len(results)) * 100:.2f}%\n")
            f.write(f"Successful analyses: {successful_analyses}\n")
            f.write(f"Total analysis time: {total_analysis_time:.3f} seconds\n")
            f.write(f"Average analysis time: {average_analysis_time:.3f} seconds\n\n")

            f.write("Individual Results:\n")
            f.write("-" * 70 + "\n")

            for result in results:
                f.write(f"File: {result['file']}\n")
                f.write(f"Result: {result['result']}\n")
                f.write(f"Analysis time: {result.get('analysis_time', 0):.3f} seconds\n")
                if result.get('error'):
                    f.write(f"Error: {result['error']}\n")
                f.write("-" * 70 + "\n")
        
        vulnerability_percentage = (vulnerable_count / len(results)) * 100 if results else 0

        print(f"\nAnalysis completed!")
        print(f"Total files processed: {len(results)}")
        print(f"Files with vulnerabilities: {vulnerable_count}")
        print(f"Vulnerability rate: {vulnerability_percentage:.2f}%")
        print(f"Successful analyses: {successful_analyses}")
        print(f"Average analysis time: {average_analysis_time:.3f} seconds")
        print(f"Report saved to: {report_path}")


def main():
    parser = argparse.ArgumentParser(description="Vulnerability analysis using AI models")
    parser.add_argument("--number", type=int, default=0,
                       help="Number of files to process (0 = all files)")
    parser.add_argument("--directory", type=str, default="data/juliet_anonymized",
                       help="Directory containing JSON files to analyze")
    parser.add_argument("--random", action="store_true",
                       help="Randomly select files instead of processing in order")
    parser.add_argument("--model", type=str, choices=["gemini", "gpt-oss-120b", "gpt-oss-20b", "gpt5", "gpt-5-mini"], default="gemini",
                       help="AI model to use for analysis (default: gemini)")
    parser.add_argument("--parallel", type=int, default=1,
                       help="Number of parallel workers for processing (default: 1)")
    parser.add_argument("--78", action="store_true",
                       help="Only process CWE78_OS_Command_Injection files")
    parser.add_argument("--type", type=str, choices=["original", "modified_1", "v4_extracted", "normalized_gpt5"], default="original",
                       help="Analysis type: 'original' for JSON files, 'modified_1' for C files func_0 analysis, 'v4_extracted' for v4 extracted JSON files, 'normalized_gpt5' for normalized C files analysis (default: original)")
    parser.add_argument("--prompt", type=int, default=0,
                       help="Prompt version to use: 0=default, N=PROMPT_N from .env (where N is any positive integer) (default: 0)")

    args = parser.parse_args()

    # Validate prompt argument
    if args.prompt < 0:
        parser.error("--prompt must be a non-negative integer (0 or positive)")

    analyzer = VulnerabilityAnalyzer(args.model, args.prompt)
    analyzer.run_analysis(args.directory, args.number, args.random, args.parallel, args.__dict__['78'], args.type)


if __name__ == "__main__":
    main()